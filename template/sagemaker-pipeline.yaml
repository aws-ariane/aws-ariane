---
AWSTemplateFormatVersion: 2010-09-09
Description: >
  CICD Pipeline for Deploying ML Microservices using Amazon SageMaker
Parameters:
  LambdasBucket:
    Description: The S3 bucket your pipeline lambdas will be strored in
    Type: String

  LambdasKey:
    Description: Key to the lambda function resouce that triggers sagemaker
    Type: String

  Email:
    Description: The Email Address that notifications are sent to
    Type: String

  GitHubUser:
    Type: String
    Description: Your username or organization name on GitHub.

  GitHubRepo:
    Type: String
    Default: tf-gamesbiz
    Description: The repo name of the sample service.

  GitHubBranch:
    Type: String
    Default: dev
    Description: The branch of the repo to continuously deploy.

  GitHubToken:
    Type: String
    NoEcho: true
    Description: >
      Token for the user specified above. (https://github.com/settings/tokens)

  BuildTimeoutMins:
    Type: Number
    Description: Time out Limit in Minutes for CodeBuild
    Default: 30

  PythonBuildVersion:
    Default: aws/codebuild/python:3.6.5-1.3.2
    Type: String
    Description: Python Version for the Linux Container in CodeBuild
    AllowedValues:
      - aws/codebuild/python:3.6.5-1.3.2

  InstanceCount:
    Description: Number of Training Instances for SageMaker
    Type: Number
    Default: 1

  InstanceType:
    Description: Type of SageMaker Compute Instance
    Type: String
    Default: ml.m4.4xlarge

  MaxRuntimeInSeconds:
    Type: Number
    Default: 86400

  VolInGB:
    Description: This is the Volume in GB for each SageMaker Instance
    Type: Number
    Default: 30

Metadata:
  AWS::CloudFormation::Interface:
    ParameterLabels:
      Email:
        default: "Email for SNS Notification"

      GitHubUser:
        default: "GitHub Username"

      GitHubRepo:
        default: "GitHub Repository"

      GitHubBranch:
        default: "Repository Branch"

      GitHubToken:
        default: "Personal Access Token"

      PythonBuildVersion:
        default: "Select which version of Python applies to CodeBuild Project"

      BuildTimeoutMins:
        default: "Enter the Timeout limit (in minutes) for CodeBuild Project"

      InstanceCount:
        default: "Number of SageMaker Training Instances"

      InstanceType:
        default: "The Type of Instance for SageMaker Training Job"

    ParameterGroups:
      - Label:
          default: Notification Configuration
        Parameters:
          - Email

      - Label:
          default: GitHub Configuration
        Parameters:
          - GitHubRepo
          - GitHubBranch
          - GitHubUser
          - GitHubToken

      - Label:
          default: AWS CodeBuild Configuration
        Parameters:
          - PythonBuildVersion
          - BuildTimeoutMins

      - Label:
          default: Amazon SageMaker Configuration
        Parameters:
          - InstanceCount
          - InstanceType
          - VolInGB
          - MaxRuntimeInSeconds

Resources:
  TrainingInputBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    DependsOn: LambdaInvokePermission
    Properties:
      BucketName: !Sub ${AWS::StackName}-input-data
      VersioningConfiguration:
        Status: Enabled
      NotificationConfiguration:
        LambdaConfigurations:
          - Function: !GetAtt LambdaPipelineTrigger.Arn
            Event: "s3:ObjectCreated:*"
            Filter:
              S3Key:
                Rules:
                  - Name: Prefix
                    Value: input/

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt 'LambdaPipelineTrigger.Arn'
      Action: 'lambda:InvokeFunction'
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub arn:aws:s3:::${AWS::StackName}-input-data

  MetaDataStore:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        - AttributeName: "training_job_name"
          AttributeType: "S"
      KeySchema:
        - AttributeName: "training_job_name"
          KeyType: "HASH"
      ProvisionedThroughput:
        ReadCapacityUnits: "10"
        WriteCapacityUnits: "5"

  TrainingImageRepository:
    Type: AWS::ECR::Repository
    DeletionPolicy: Retain

  InferenceImageRepository:
    Type: AWS::ECR::Repository
    DeletionPolicy: Retain

  CodepipelineArtifactBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      BucketName: !Sub ${AWS::StackName}-artifact-store
      VersioningConfiguration:
        Status: Enabled

  ModelArtifactBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      BucketName: !Sub ${AWS::StackName}-model-output
      VersioningConfiguration:
        Status: Enabled

  PipelineEndSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub ${AWS::StackName}-end-topic
      Subscription:
        - Endpoint: !Ref Email
          Protocol: email

  PipelineServiceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-pipeline-role
      AssumeRolePolicyDocument:
        Statement:
        - Action: ['sts:AssumeRole']
          Effect: Allow
          Principal:
            Service: [codepipeline.amazonaws.com]
        Version: '2012-10-17'
      Path: /
      Policies:
        - PolicyName: CodePipelineAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                - s3:*
                - codebuild:*
                - cloudformation:CreateStack
                - cloudformation:DescribeStacks
                - cloudformation:DeleteStack
                - cloudformation:UpdateStack
                - cloudformation:CreateChangeSet
                - cloudformation:ExecuteChangeSet
                - cloudformation:DeleteChangeSet
                - cloudformation:DescribeChangeSet
                - cloudformation:SetStackPolicy
                - lambda:*
                - iam:PassRole
                - sns:Publish
                Effect: Allow
                Resource: '*'

  CustomS3FolderHandler:
    Type: Custom::CustomS3FolderHandler
    DependsOn: TrainingInputBucket
    Properties:
      ServiceToken: !GetAtt LambdaS3FolderFunction.Arn
      TrainingInputS3: !Ref TrainingInputBucket
      LambdasBucketS3: !Ref LambdasBucket

  LambdaS3FolderFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: On stack creation sets up the Sagemaker equivalent folder structure in Training input S3 Bucket
      Handler: index.lambda_handler
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
          import json
          import os
          import boto3
          from botocore.vendored import requests
          import cfnresponse

          s3 = boto3.client('s3')

          def lambda_handler(event, context):

            input_bucket_name = str(event['ResourceProperties']['TrainingInputS3'])
            lambdas_bucket_name = str(event['ResourceProperties']['LambdasBucketS3'])

            if(event['RequestType'] == "Create"):

              input_directory_path2 = "input/data/training" + "/"
              input_directory_path3 = "input/data/validation" + "/"
              input_directory_path4 = "input/data/testing" + "/"
              input_directory_path1 = "input/config" + "/"

              s3.put_object(Bucket=input_bucket_name, Key=input_directory_path1 )
              s3.put_object(Bucket=input_bucket_name, Key=input_directory_path2 )
              s3.put_object(Bucket=input_bucket_name, Key=input_directory_path3 )
              s3.put_object(Bucket=input_bucket_name, Key=input_directory_path4 )

              responseData = {}
              cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
            else:
              print("got till here")
              cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData=None)

  LambdaCheckInputs:
    Type: AWS::Lambda::Function
    Properties:
      Description: Checks if CodePipeline pipeline exists and then manually executes it.
      Environment:
        Variables:
          'PIPELINE': !Ref 'AWS::StackName'
      Handler: index.lambda_handler
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
         import os
         import json
         import boto3
         import logging

  LambdaPipelineTrigger:
    Type: AWS::Lambda::Function
    Properties:
      Description: Checks if CodePipeline pipeline exists and then manually executes it.
      Environment:
        Variables:
          'PIPELINE': !Ref 'AWS::StackName'
      Handler: index.lambda_handler
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        ZipFile: |
         import os
         import json
         import boto3
         import logging

         codepipeline = boto3.client('codepipeline')

         logger = logging.getLogger()
         logger.setLevel(logging.INFO)


         def lambda_handler(event, context):
             logger.info('got event {}'.format(event))

             pipelineName = str(os.environ["PIPELINE"])

             response1 = codepipeline.list_pipeline_executions(pipelineName=str(pipelineName))

             if not response1['pipelineExecutionSummaries'][0]:
                 print('Kick off a new pipeline execution')
                 codepipeline.start_pipeline_execution(pipelineName=str(pipelineName))
             else:
                 latest_execution = response1['pipelineExecutionSummaries'][0]
                 execution_status = str(latest_execution['status'])

                 if execution_status == 'InProgress':
                     print('Execution In Progress Do Nothing')

                 if execution_status == 'Succeeded':
                     print('Restarting execution for Pipeline')
                     codepipeline.start_pipeline_execution(name=str(pipelineName))

                 if execution_status == 'Failed':
                     print('Restarting Failed execution for Pipeline')
                     codepipeline.start_pipeline_execution(name=str(pipelineName))

  CodeBuildTrainingProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: CODEPIPELINE
      Cache:
        Type: NO_CACHE
      Source:
        Type: CODEPIPELINE
        GitCloneDepth: 1
        BuildSpec: |
          version: 0.2
          phases:
            install:
              commands:
                - pip install --upgrade pip
            pre_build:
              commands:
                - export IMG="${REPO_NAME}"
                - export PAYLOAD={\"COMMIT_ID\":\"${CODEBUILD_RESOLVED_SOURCE_VERSION}\",\"IMG\":\"${REPO_NAME}\"}
                - echo "${PAYLOAD}" > outfile.txt
                - python setup.py egg_info
                - export pkg_name=$(ps -ef | awk '{ if ($1 ~ /Name:/) print $2}' *.egg-info/PKG-INFO)
                - export pkg_version=$(ps -ef | awk '{ if ($1 ~ /^Version:/) print $2}' *.egg-info/PKG-INFO)
                - module_name=$(ps -ef | awk '{ if ($1 ~ /train/) print $3 }' *.egg-info/entry_points.txt | cut -d ':' -f1 | sed -e 's/^"//' -e 's/"$//')
                - function_name=$(ps -ef | awk '{ if ($1 ~ /train/) print $3 }' *.egg-info/entry_points.txt | cut -d ':' -f2 | sed -e 's/^"//' -e 's/"$//')
                - export module_name=$(echo $module_name)
                - export function_name=$(echo $function_name)
                - export dependencies=""
                - |
                  while read -r line
                  do
                       dependencies="$dependencies""$line "
                  done < "${pkg_name}.egg-info/requires.txt"
                - python setup.py sdist
                - cd dist
                - echo "# ------------------------------------------------------------------------------" >> Dockerfile
                - echo "# Main" >> Dockerfile
                - echo "# =======" >> Dockerfile
                - echo "# Can be used as base image for regular training job executions" >> Dockerfile
                - echo "# ------------------------------------------------------------------------------" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "FROM python:3.6-slim" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Add standard build tools and libraries" >> Dockerfile
                - echo "RUN apt-get -y update && apt-get install -y --no-install-recommends \ " >> Dockerfile
                - echo "    build-essential \ " >> Dockerfile
                - echo "    unixodbc-dev \ " >> Dockerfile
                - echo "    nginx \ " >> Dockerfile
                - echo "    libboost-python-dev \ " >> Dockerfile
                - echo "    ca-certificates" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Install any package-specific python requirements" >> Dockerfile
                - echo "RUN pip install $dependencies" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Copy over entrypoint script and define image entry"
                - echo "COPY run.py /opt/run.py" >> Dockerfile
                - echo "ENTRYPOINT ["'"python"'", "'"/opt/run.py"'"]" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Copy and install training code" >> Dockerfile
                - echo "COPY $pkg_name-$pkg_version.tar.gz /opt/$pkg_name-$pkg_version.tar.gz" >> Dockerfile
                - echo "RUN pip install /opt/$pkg_name-$pkg_version.tar.gz" >> Dockerfile
                - echo "from $module_name import $function_name" >> run.py
                - echo "$function_name()" >> run.py
                - cat run.py
                - cat Dockerfile
                - cd ..
            build:
              commands:
                - $(aws ecr get-login --region "${AWS_DEFAULT_REGION}" --no-include-email)
                - docker build -t "${IMG}" ./dist
                - docker tag "${IMG}" "${FULL_NAME}"
            post_build:
              commands:
                - docker push "${FULL_NAME}"
          artifacts:
            files:
              - outfile.txt
            discard-paths: yes
      Environment:
        ComputeType: BUILD_GENERAL1_LARGE
        Image: !Ref PythonBuildVersion
        Type: LINUX_CONTAINER
        PrivilegedMode: true
        EnvironmentVariables:
          - Name: AWS_DEFAULT_REGION
            Value: !Ref AWS::Region
          - Name: FULL_NAME
            Value: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${TrainingImageRepository}:latest
          - Name: REPO_NAME
            Value: !Sub ${GitHubRepo}
      Name: !Sub ${AWS::StackName}-train
      ServiceRole: !Ref CodeBuildServiceRole
      TimeoutInMinutes: !Ref BuildTimeoutMins

  CodeBuildInferenceProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: CODEPIPELINE
      Cache:
        Type: NO_CACHE
      Source:
        Type: CODEPIPELINE
        GitCloneDepth: 1
        BuildSpec: |
          version: 0.2
          phases:
            install:
              commands:
                - pip install --upgrade pip
            pre_build:
              commands:
                - export IMG="${REPO_NAME}"
                - export PAYLOAD={\"COMMIT_ID\":\"${CODEBUILD_RESOLVED_SOURCE_VERSION}\",\"IMG\":\"${REPO_NAME}\"}
                - echo "${PAYLOAD}" > outfile.txt
                - python setup.py egg_info
                - export pkg_name=$(ps -ef | awk '{ if ($1 ~ /Name:/) print $2}' *.egg-info/PKG-INFO)
                - export pkg_version=$(ps -ef | awk '{ if ($1 ~ /^Version:/) print $2}' *.egg-info/PKG-INFO)
                - module_name=$(ps -ef | awk '{ if ($1 ~ /serve/) print $3 }' *.egg-info/entry_points.txt | cut -d ':' -f1 | sed -e 's/^"//' -e 's/"$//')
                - function_name=$(ps -ef | awk '{ if ($1 ~ /serve/) print $3 }' *.egg-info/entry_points.txt | cut -d ':' -f2 | sed -e 's/^"//' -e 's/"$//')
                - export module_name=$(echo $module_name)
                - export function_name=$(echo $function_name)
                - export dependencies=""
                - |
                  while read -r line
                  do
                       dependencies="$dependencies""$line "
                  done < "${pkg_name}.egg-info/requires.txt"
                - python setup.py sdist
                - cd dist
                - echo "# ------------------------------------------------------------------------------" >> Dockerfile
                - echo "# Main" >> Dockerfile
                - echo "# =======" >> Dockerfile
                - echo "# Can be used as base image for regular inference job executions" >> Dockerfile
                - echo "# ------------------------------------------------------------------------------" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "FROM python:3.6-slim" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Add standard build tools and libraries" >> Dockerfile
                - echo "RUN apt-get -y update && apt-get install -y --no-install-recommends \ " >> Dockerfile
                - echo "    build-essential \ " >> Dockerfile
                - echo "    unixodbc-dev \ " >> Dockerfile
                - echo "    nginx \ " >> Dockerfile
                - echo "    libboost-python-dev \ " >> Dockerfile
                - echo "    ca-certificates" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Install any package-specific python requirements" >> Dockerfile
                - echo "RUN pip install $dependencies" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Copy over entrypoint script and define image entry"
                - echo "COPY run.py /opt/run.py" >> Dockerfile
                - echo "ENTRYPOINT ["'"python"'", "'"/opt/run.py"'"]" >> Dockerfile
                - echo "" >> Dockerfile
                - echo "# Copy and install inference code" >> Dockerfile
                - echo "COPY $pkg_name-$pkg_version.tar.gz /opt/$pkg_name-$pkg_version.tar.gz" >> Dockerfile
                - echo "RUN pip install /opt/$pkg_name-$pkg_version.tar.gz" >> Dockerfile
                - echo "from $module_name import $function_name" >> run.py
                - echo "$function_name()" >> run.py
                - cat run.py
                - cat Dockerfile
                - cd ..
            build:
              commands:
                - $(aws ecr get-login --region "${AWS_DEFAULT_REGION}" --no-include-email)
                - docker build -t "${IMG}" ./dist
                - docker tag "${IMG}" "${FULL_NAME}"
            post_build:
              commands:
                - docker push "${FULL_NAME}"
          artifacts:
            files:
              - outfile.txt
            discard-paths: yes
      Environment:
        ComputeType: BUILD_GENERAL1_LARGE
        Image: !Ref PythonBuildVersion
        Type: LINUX_CONTAINER
        PrivilegedMode: true
        EnvironmentVariables:
          - Name: AWS_DEFAULT_REGION
            Value: !Ref AWS::Region
          - Name: FULL_NAME
            Value: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${InferenceImageRepository}:latest
          - Name: REPO_NAME
            Value: !Sub ${GitHubRepo}
      Name: !Sub ${AWS::StackName}-infer
      ServiceRole: !Ref CodeBuildServiceRole
      TimeoutInMinutes: !Ref BuildTimeoutMins

  LambdaSageMakerTrigger:
    Type: AWS::Lambda::Function
    Properties:
      Description: Writes to DynamoDB and calls the SageMaker CreateTrainingJob API
      Environment:
        Variables:
          'ECR_REPO': !Ref TrainingImageRepository
          'SRC_BKT_NAME': !Ref TrainingInputBucket
          'META_DATA_STORE': !Ref MetaDataStore
          'IMG': !Sub ${GitHubRepo}
          'FULL_NAME': !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${TrainingImageRepository}:latest
          'SAGE_ROLE_ARN': !GetAtt 'LambdaServiceRole.Arn'
          'INSTANCE_TYPE': !Ref InstanceType
          'INSTANCE_CNT': !Ref InstanceCount
          'EBS_VOL_GB': !Ref VolInGB
          'RUN_TIME_SEC': !Ref MaxRuntimeInSeconds
          'SRC_BKT_URI': !Sub s3://${TrainingInputBucket}/input/data/
          'DEST_BKT_URI': !Sub s3://${ModelArtifactBucket}/
      Handler: sagemaker-trigger.main
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        S3Bucket: !Ref LambdasBucket
        S3Key: !Ref LambdasKey

  LambdaDeployTrigger:
    Type: AWS::Lambda::Function
    Properties:
      Description: This is triggered by training job action being completed and kicking off Sagemaker hosting
      Environment:
        Variables:
          'META_DATA_STORE': !Ref MetaDataStore
          'DEST_BKT': !Sub ${ModelArtifactBucket}
          'FULL_NAME': !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${InferenceImageRepository}:latest
          'IMG': !Sub ${GitHubRepo}
          'SAGE_ROLE_ARN': !GetAtt 'LambdaServiceRole.Arn'
      Handler: sagemaker-deploy-trigger.main
      Runtime: python3.6
      Timeout: '300'
      Role: !GetAtt 'LambdaServiceRole.Arn'
      Code:
        S3Bucket: !Ref LambdasBucket
        S3Key: !Ref LambdasKey

  Pipeline:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      ArtifactStore:
        Location: !Ref 'CodepipelineArtifactBucket'
        Type: S3
      DisableInboundStageTransitions: []
      Name: !Ref 'AWS::StackName'
      RoleArn: !GetAtt [PipelineServiceRole, Arn]
      Stages:
        - Name: Source
          Actions:
            - Name: Source
              ActionTypeId:
                Category: Source
                Owner: ThirdParty
                Provider: GitHub
                Version: '1'
              Configuration:
                Owner: !Ref 'GitHubUser'
                Repo: !Ref 'GitHubRepo'
                Branch: !Ref 'GitHubBranch'
                OAuthToken: !Ref 'GitHubToken'
              OutputArtifacts:
                - Name: src
              RunOrder: '1'
        - Name: Build
          Actions:
            - Name: Build_Training_Image
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              Configuration:
                ProjectName: !Ref 'CodeBuildTrainingProject'
              InputArtifacts:
                - Name: src
              OutputArtifacts:
                - Name: bld
              RunOrder: '1'
            - Name: Build_Inference_Image
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: '1'
              Configuration:
                ProjectName: !Ref 'CodeBuildInferenceProject'
              InputArtifacts:
                - Name: src
              OutputArtifacts:
                - Name: bldi
              RunOrder: '1'
        - Name: SageMaker
          Actions:
            - Name: SageMaker_Train
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: '1'
              Configuration:
                FunctionName: !Ref 'LambdaSageMakerTrigger'
              InputArtifacts:
                - Name: bld
              OutputArtifacts:
                - Name: trigger
              RunOrder: '1'
            - Name: SageMaker_Deploy
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: '1'
              Configuration:
                FunctionName: !Ref 'LambdaDeployTrigger'
              InputArtifacts:
                - Name: trigger
              RunOrder: '2'
        - Name: Production
          Actions:
            - Name: ApprovalGate
              ActionTypeId:
                Category: Approval
                Owner: AWS
                Provider: Manual
                Version: '1'
              Configuration:
                NotificationArn: !Ref PipelineEndSNSTopic
                CustomData: !Sub 'Do you want to push your changes to production?'
              RunOrder: '1'

  CodeBuildServiceRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codebuild.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Resource: "*"
                Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - ecr:GetAuthorizationToken
                  - lambda:*
                  - iam:PassRole
              - Resource: !Sub arn:aws:s3:::${TrainingInputBucket}*
                Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:GetObjectVersion
                  - s3:ListObjects
              - Resource: !Sub arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/*
                Effect: Allow
                Action:
                  - ecr:GetDownloadUrlForLayer
                  - ecr:BatchGetImage
                  - ecr:BatchCheckLayerAvailability
                  - ecr:PutImage
                  - ecr:InitiateLayerUpload
                  - ecr:UploadLayerPart
                  - ecr:CompleteLayerUpload
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
        - arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess

  LambdaServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
                - sagemaker.amazonaws.com
                - codepipeline.amazonaws.com
                - codebuild.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: LambdaPipelineRole
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Resource: "*"
                Effect: Allow
                Action:
                  - ec2:DescribeVpcs
                  - ec2:DescribeSubnets
                  - ec2:DescribeSecurityGroups
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DescribeDhcpOptions
                  - ec2:DeleteNetworkInterfacePermission
                  - ec2:CreateNetworkInterfacePermission
                  - ec2:CreateNetworkInterface
                  - ec2:DeleteNetworkInterface
                  - ec2:DetachNetworkInterface
                  - ec2:ModifyNetworkInterfaceAttribute
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:GetLogEvents
                  - logs:PutLogEvents
                  - iam:PassRole
                  - sns:Publish
                  - lambda:InvokeFunction
                  - cloudformation:DescribeStacks
                  - codepipeline:*
                  - codepipeline:PutJobSuccessResult
                  - codepipeline:PutJobFailureResult
                  - logs:*
                  - s3:*
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
        - arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
        - arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/AWSCodePipelineFullAccess

Outputs:
  PipelineUrl:
    Value: !Sub https://console.aws.amazon.com/codepipeline/home?region=${AWS::Region}#/view/${Pipeline}
    Description: CodePipeline URL

  ModelArtifactBucket:
    Description: >
      SageMaker Model Output S3 bucket
    Value: !Sub https://console.aws.amazon.com/s3/buckets/${ModelArtifactBucket}/?region=${AWS::Region}

  TrainingInputBucket:
    Description: >
      SageMaker Training Input S3 bucket
    Value: !Sub https://console.aws.amazon.com/s3/buckets/${TrainingInputBucket}/?region=${AWS::Region}

  MetaDataStore:
    Description: >
      DynamoDB Table Acting as Meta Data Store for Training job execution
    Value: !Sub https://console.aws.amazon.com/dynamodb/home?region=${AWS::Region}#tables:selected=${MetaDataStore};tab=items
